{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_c76FtRr2QME"
   },
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Hayden Daly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkWorQAI2MW2"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJbYXou6chZf",
    "outputId": "6ada05b7-04bb-47f1-f76f-9c4143f07af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 20 18:14:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtyNWBDi2bSW"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tJ9MEy72dXr"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEx4z-Vy2gKY",
    "outputId": "5defeb4a-b466-4779-b505-6449bbfb10c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX7dYnBN2m5M"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o5NMIpk2oNO",
    "outputId": "31431585-62a8-49e6-e769-024ba51cad80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    l, _ = y.shape\n",
    "    new_y = numpy.zeros((l, num_class))\n",
    "    for i in range(l):\n",
    "        new_y[i][y[i]] = 1\n",
    "    return new_y\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENyn9xeX2wa8"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCMpPvWC22P1"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex2Mt4az24NB",
    "outputId": "ddc0126a-b8d0-4501-d4ad-608ee781ccce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp7U06eB2_rr"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABXzfK6y3CVV"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jednpu-b3AiT",
    "outputId": "995ee80c-b2a7-4930-f2c0-93f32cd0bbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                11530     \n",
      "=================================================================\n",
      "Total params: 300,330\n",
      "Trainable params: 299,434\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VDb9pPE83Ij7"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4\n",
    "decay = 1E-6\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate, decay=decay),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hABC80no3Kan",
    "outputId": "382ce9d6-6bf6-4db9-a65e-9bf6ac08d9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "625/625 [==============================] - 7s 8ms/step - loss: 0.1072 - acc: 0.9627 - val_loss: 0.1640 - val_acc: 0.9428\n",
      "Epoch 2/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1065 - acc: 0.9621 - val_loss: 0.1689 - val_acc: 0.9427\n",
      "Epoch 3/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1038 - acc: 0.9638 - val_loss: 0.1757 - val_acc: 0.9375\n",
      "Epoch 4/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1048 - acc: 0.9626 - val_loss: 0.1812 - val_acc: 0.9390\n",
      "Epoch 5/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1022 - acc: 0.9649 - val_loss: 0.1800 - val_acc: 0.9383\n",
      "Epoch 6/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1018 - acc: 0.9621 - val_loss: 0.1833 - val_acc: 0.9380\n",
      "Epoch 7/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1007 - acc: 0.9630 - val_loss: 0.1832 - val_acc: 0.9411\n",
      "Epoch 8/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1034 - acc: 0.9627 - val_loss: 0.1831 - val_acc: 0.9350\n",
      "Epoch 9/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0972 - acc: 0.9654 - val_loss: 0.2072 - val_acc: 0.9301\n",
      "Epoch 10/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1040 - acc: 0.9632 - val_loss: 0.1988 - val_acc: 0.9348\n",
      "Epoch 11/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0994 - acc: 0.9643 - val_loss: 0.2073 - val_acc: 0.9314\n",
      "Epoch 12/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1052 - acc: 0.9625 - val_loss: 0.1922 - val_acc: 0.9365\n",
      "Epoch 13/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0951 - acc: 0.9637 - val_loss: 0.1986 - val_acc: 0.9364\n",
      "Epoch 14/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1014 - acc: 0.9627 - val_loss: 0.2034 - val_acc: 0.9310\n",
      "Epoch 15/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1002 - acc: 0.9635 - val_loss: 0.2356 - val_acc: 0.9233\n",
      "Epoch 16/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0964 - acc: 0.9654 - val_loss: 0.1976 - val_acc: 0.9362\n",
      "Epoch 17/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1009 - acc: 0.9645 - val_loss: 0.2206 - val_acc: 0.9288\n",
      "Epoch 18/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1036 - acc: 0.9633 - val_loss: 0.2137 - val_acc: 0.9293\n",
      "Epoch 19/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1005 - acc: 0.9641 - val_loss: 0.2053 - val_acc: 0.9323\n",
      "Epoch 20/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1035 - acc: 0.9626 - val_loss: 0.2131 - val_acc: 0.9286\n",
      "Epoch 21/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1057 - acc: 0.9631 - val_loss: 0.2486 - val_acc: 0.9194\n",
      "Epoch 22/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0974 - acc: 0.9652 - val_loss: 0.2290 - val_acc: 0.9252\n",
      "Epoch 23/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1002 - acc: 0.9634 - val_loss: 0.2531 - val_acc: 0.9190\n",
      "Epoch 24/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0955 - acc: 0.9661 - val_loss: 0.2277 - val_acc: 0.9256\n",
      "Epoch 25/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0951 - acc: 0.9666 - val_loss: 0.2320 - val_acc: 0.9252\n",
      "Epoch 26/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1007 - acc: 0.9635 - val_loss: 0.2288 - val_acc: 0.9268\n",
      "Epoch 27/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1022 - acc: 0.9650 - val_loss: 0.2668 - val_acc: 0.9192\n",
      "Epoch 28/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0979 - acc: 0.9668 - val_loss: 0.2392 - val_acc: 0.9231\n",
      "Epoch 29/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0976 - acc: 0.9642 - val_loss: 0.2356 - val_acc: 0.9229\n",
      "Epoch 30/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0901 - acc: 0.9678 - val_loss: 0.2405 - val_acc: 0.9215\n",
      "Epoch 31/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1020 - acc: 0.9637 - val_loss: 0.2415 - val_acc: 0.9220\n",
      "Epoch 32/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0978 - acc: 0.9650 - val_loss: 0.2565 - val_acc: 0.9142\n",
      "Epoch 33/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0945 - acc: 0.9663 - val_loss: 0.2505 - val_acc: 0.9174\n",
      "Epoch 34/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0910 - acc: 0.9668 - val_loss: 0.2669 - val_acc: 0.9162\n",
      "Epoch 35/35\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0898 - acc: 0.9671 - val_loss: 0.2567 - val_acc: 0.9172\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=64, epochs=35, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "h_2fGfoe3OoB",
    "outputId": "c4a42c2a-09e0-4804-8870-447408c6d51e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTbLANpE3WD3"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XlksOth3avz"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uFdR-Hx73cqn"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4\n",
    "decay = 1E-6\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate, decay=decay),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tc_qILCm3fiU",
    "outputId": "eb4aa893-a750-4679-8282-b3528f002a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "782/782 [==============================] - 8s 8ms/step - loss: 0.1933 - acc: 0.9411\n",
      "Epoch 2/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1765 - acc: 0.9437\n",
      "Epoch 3/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1678 - acc: 0.9454\n",
      "Epoch 4/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1565 - acc: 0.9485\n",
      "Epoch 5/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1578 - acc: 0.9471\n",
      "Epoch 6/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1539 - acc: 0.9477\n",
      "Epoch 7/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1547 - acc: 0.9480\n",
      "Epoch 8/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1457 - acc: 0.9512\n",
      "Epoch 9/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1457 - acc: 0.9489\n",
      "Epoch 10/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1403 - acc: 0.9522\n",
      "Epoch 11/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1361 - acc: 0.9527\n",
      "Epoch 12/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1386 - acc: 0.9531\n",
      "Epoch 13/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1373 - acc: 0.9516\n",
      "Epoch 14/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1411 - acc: 0.9522\n",
      "Epoch 15/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1375 - acc: 0.9519\n",
      "Epoch 16/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1327 - acc: 0.9533\n",
      "Epoch 17/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1318 - acc: 0.9550\n",
      "Epoch 18/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1334 - acc: 0.9528\n",
      "Epoch 19/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1317 - acc: 0.9549\n",
      "Epoch 20/35\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.1306 - acc: 0.9543\n",
      "Epoch 21/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1338 - acc: 0.9525\n",
      "Epoch 22/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1252 - acc: 0.9557\n",
      "Epoch 23/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1295 - acc: 0.9544\n",
      "Epoch 24/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1300 - acc: 0.9546\n",
      "Epoch 25/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1233 - acc: 0.9566\n",
      "Epoch 26/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1289 - acc: 0.9545\n",
      "Epoch 27/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1252 - acc: 0.9556\n",
      "Epoch 28/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1158 - acc: 0.9592\n",
      "Epoch 29/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1217 - acc: 0.9566\n",
      "Epoch 30/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1221 - acc: 0.9586\n",
      "Epoch 31/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1224 - acc: 0.9566\n",
      "Epoch 32/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1182 - acc: 0.9571\n",
      "Epoch 33/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1200 - acc: 0.9582\n",
      "Epoch 34/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1221 - acc: 0.9569\n",
      "Epoch 35/35\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1190 - acc: 0.9582\n"
     ]
    }
   ],
   "source": [
    "# <Train your model on the entire training set (50K samples)>\n",
    "# <Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "# <Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "history = model.fit(x_train, y_train_vec, batch_size=64, epochs=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n9VMtxl3lAf"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybxbwEtO-N3_",
    "outputId": "a1678de9-04b9-443d-fca9-36e1ed598570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5992 - acc: 0.8488\n",
      "loss = 0.5992285013198853\n",
      "accuracy = 0.848800003528595\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HM4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
